"""
10 - ç”Ÿäº§çº§ MinIO Pipeline

å­¦ä¹ ç›®æ ‡ï¼š
1. æ„å»ºç”Ÿäº§çº§æ•°æ®åŠ è½½æµæ°´çº¿
2. å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
3. æ·»åŠ æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
4. å®ç°ç¼“å­˜å’Œé¢„å–ç­–ç•¥
5. ä¸å®Œæ•´è®­ç»ƒå¾ªç¯é›†æˆ

æ ¸å¿ƒæ¦‚å¿µï¼š
- Production-ready pipeline
- Error handling and retry
- Performance monitoring
- Caching strategies
- Multi-GPU support
"""

import nvidia.dali as dali
from nvidia.dali import pipeline_def
import nvidia.dali.fn as fn
import nvidia.dali.types as types
from nvidia.dali.plugin.pytorch import DALIGenericIterator

import torch
import torch.nn as nn
import numpy as np
from minio import Minio
from minio.error import S3Error
import io
from PIL import Image
import time
import logging
from collections import deque
import threading


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MinIODataSourceWithCache:
    """
    å¸¦ç¼“å­˜çš„ç”Ÿäº§çº§ MinIO æ•°æ®æº

    Features:
    - é”™è¯¯å¤„ç†å’Œé‡è¯•
    - LRU ç¼“å­˜
    - æ€§èƒ½ç›‘æ§
    - çº¿ç¨‹å®‰å…¨
    """
    def __init__(
        self,
        client,
        bucket_name,
        object_names,
        cache_size=100,
        max_retries=3,
        retry_delay=1.0
    ):
        """
        Args:
            client: MinIO å®¢æˆ·ç«¯
            bucket_name: bucket åç§°
            object_names: å¯¹è±¡åç§°åˆ—è¡¨
            cache_size: ç¼“å­˜å¤§å°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.client = client
        self.bucket_name = bucket_name
        self.object_names = object_names
        self.num_samples = len(object_names)
        self.max_retries = max_retries
        self.retry_delay = retry_delay

        # LRU ç¼“å­˜
        self.cache = {}
        self.cache_size = cache_size
        self.cache_order = deque(maxlen=cache_size)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0,
            'retries': 0
        }

        # çº¿ç¨‹é”
        self.lock = threading.Lock()

        logger.info(f"Initialized MinIODataSource: {self.num_samples} samples, cache_size={cache_size}")

    def _get_from_cache(self, key):
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        with self.lock:
            if key in self.cache:
                self.stats['cache_hits'] += 1
                # æ›´æ–° LRU é¡ºåº
                self.cache_order.remove(key)
                self.cache_order.append(key)
                return self.cache[key]

            self.stats['cache_misses'] += 1
            return None

    def _add_to_cache(self, key, value):
        """æ·»åŠ æ•°æ®åˆ°ç¼“å­˜"""
        with self.lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§çš„é¡¹
            if len(self.cache) >= self.cache_size and key not in self.cache:
                if self.cache_order:
                    oldest_key = self.cache_order.popleft()
                    del self.cache[oldest_key]

            self.cache[key] = value
            if key not in self.cache_order:
                self.cache_order.append(key)

    def _download_with_retry(self, object_name):
        """
        å¸¦é‡è¯•çš„ä¸‹è½½

        Args:
            object_name: å¯¹è±¡åç§°

        Returns:
            å›¾åƒæ•°æ®ï¼ˆNumPy arrayï¼‰

        Raises:
            Exception: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        """
        for attempt in range(self.max_retries):
            try:
                response = self.client.get_object(self.bucket_name, object_name)
                image_data = response.read()
                response.close()
                response.release_conn()

                # è§£ç å›¾åƒ
                img = Image.open(io.BytesIO(image_data))
                img_array = np.array(img, dtype=np.uint8)

                return img_array

            except S3Error as e:
                self.stats['retries'] += 1
                logger.warning(f"Attempt {attempt + 1}/{self.max_retries} failed for {object_name}: {e}")

                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    self.stats['errors'] += 1
                    raise

    def __call__(self, sample_info):
        """DALI å›è°ƒå‡½æ•°"""
        idx = sample_info.idx_in_epoch

        if idx >= self.num_samples:
            raise StopIteration

        self.stats['total_requests'] += 1
        object_name = self.object_names[idx]

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_data = self._get_from_cache(object_name)
        if cached_data is not None:
            return cached_data

        # ä¸‹è½½æ•°æ®
        try:
            img_array = self._download_with_retry(object_name)

            # ç”Ÿæˆæ ‡ç­¾ï¼ˆå®é™…åº”ç”¨ä¸­ä»å…ƒæ•°æ®è¯»å–ï¼‰
            label = idx % 10

            result = (img_array, np.array([label], dtype=np.int32))

            # æ·»åŠ åˆ°ç¼“å­˜
            self._add_to_cache(object_name, result)

            return result

        except Exception as e:
            logger.error(f"Failed to load {object_name}: {e}")
            # è¿”å›ç©ºå›¾åƒä½œä¸ºåå¤‡
            return (
                np.zeros((224, 224, 3), dtype=np.uint8),
                np.array([0], dtype=np.int32)
            )

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            stats = self.stats.copy()

        if stats['total_requests'] > 0:
            stats['cache_hit_rate'] = stats['cache_hits'] / stats['total_requests']
        else:
            stats['cache_hit_rate'] = 0.0

        return stats

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            self.stats = {
                'total_requests': 0,
                'cache_hits': 0,
                'cache_misses': 0,
                'errors': 0,
                'retries': 0
            }


@pipeline_def
def production_minio_pipeline(minio_source, image_size=224, is_training=True):
    """
    ç”Ÿäº§çº§ MinIO Pipeline

    Args:
        minio_source: MinIO æ•°æ®æº
        image_size: å›¾åƒå¤§å°
        is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
    """
    # è¯»å–æ•°æ®
    images, labels = fn.external_source(
        source=minio_source,
        num_outputs=2,
        dtype=[types.UINT8, types.INT32],
        batch=False
    )

    if is_training:
        # è®­ç»ƒæ¨¡å¼ï¼šæ•°æ®å¢å¼º
        images = fn.random_resized_crop(
            images,
            size=image_size,
            random_area=[0.08, 1.0],
            random_aspect_ratio=[0.75, 1.33]
        )

        images = fn.flip(
            images,
            horizontal=fn.random.coin_flip(probability=0.5)
        )

        images = fn.brightness_contrast(
            images,
            brightness=fn.random.uniform(range=[0.8, 1.2]),
            contrast=fn.random.uniform(range=[0.8, 1.2])
        )

    else:
        # éªŒè¯æ¨¡å¼ï¼šä¸­å¿ƒè£å‰ª
        images = fn.resize(images, size=int(image_size * 1.14))
        images = fn.crop(images, crop=image_size, crop_pos_x=0.5, crop_pos_y=0.5)

    # å½’ä¸€åŒ–
    images = fn.cast(images, dtype=types.FLOAT) / 255.0
    images = fn.normalize(
        images,
        mean=[0.485, 0.456, 0.406],
        stddev=[0.229, 0.224, 0.225],
        axes=(2,)
    )

    # CHW æ ¼å¼
    images = fn.transpose(images, perm=[2, 0, 1])

    return images, labels


def demo_production_pipeline():
    """æ¼”ç¤ºç”Ÿäº§çº§ Pipeline"""
    print("\n" + "="*60)
    print("Production Pipeline Demo")
    print("="*60)

    # è®¾ç½® MinIO å®¢æˆ·ç«¯
    try:
        client = Minio(
            "localhost:9000",
            access_key="minioadmin",
            secret_key="minioadmin",
            secure=False
        )
        logger.info("Connected to MinIO")
    except Exception as e:
        logger.error(f"Could not connect to MinIO: {e}")
        print("\nâš ï¸  Please ensure MinIO is running on localhost:9000")
        return

    # è·å–å¯¹è±¡åˆ—è¡¨
    bucket_name = "dali-tutorial"
    try:
        objects = list(client.list_objects(bucket_name, prefix="images/", recursive=True))
        object_names = [obj.object_name for obj in objects]

        if not object_names:
            logger.warning(f"No objects found in {bucket_name}/images/")
            print("\nâš ï¸  Please run 09_minio_basic.py first to create sample data")
            return

        logger.info(f"Found {len(object_names)} objects")

    except S3Error as e:
        logger.error(f"Error accessing bucket: {e}")
        return

    # åˆ›å»ºæ•°æ®æº
    data_source = MinIODataSourceWithCache(
        client=client,
        bucket_name=bucket_name,
        object_names=object_names,
        cache_size=50,
        max_retries=3
    )

    # åˆ›å»º Pipeline
    pipe = production_minio_pipeline(
        minio_source=data_source,
        image_size=224,
        is_training=True,
        batch_size=8,
        num_threads=4,
        device_id=0,
        prefetch_queue_depth=2  # é¢„å–æ·±åº¦
    )
    pipe.build()

    logger.info("Pipeline built successfully")

    # åˆ›å»ºè¿­ä»£å™¨
    dali_iter = DALIGenericIterator(
        pipelines=[pipe],
        output_map=["images", "labels"],
        size=len(object_names),
        auto_reset=True
    )

    # æ€§èƒ½æµ‹è¯•
    print("\nPerformance Test:")
    num_epochs = 2
    start_time = time.time()

    for epoch in range(num_epochs):
        epoch_start = time.time()

        for i, batch in enumerate(dali_iter):
            data = batch[0]
            images = data["images"]
            labels = data["labels"]

            # æ¨¡æ‹Ÿè®­ç»ƒ
            time.sleep(0.001)

        epoch_time = time.time() - epoch_start
        logger.info(f"Epoch {epoch + 1}/{num_epochs} completed in {epoch_time:.2f}s")

    total_time = time.time() - start_time
    total_images = len(object_names) * num_epochs
    throughput = total_images / total_time

    print(f"\nResults:")
    print(f"  - Total time: {total_time:.2f}s")
    print(f"  - Throughput: {throughput:.0f} images/sec")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = data_source.get_stats()
    print(f"\nCache Statistics:")
    print(f"  - Total requests: {stats['total_requests']}")
    print(f"  - Cache hits: {stats['cache_hits']}")
    print(f"  - Cache misses: {stats['cache_misses']}")
    print(f"  - Cache hit rate: {stats['cache_hit_rate']:.2%}")
    print(f"  - Errors: {stats['errors']}")
    print(f"  - Retries: {stats['retries']}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("DALI Tutorial 10: Production MinIO Pipeline")
    print("="*60)

    demo_production_pipeline()

    print("\n" + "="*60)
    print("âœ“ Tutorial 10 completed!")
    print("="*60)
    print("\nKey Takeaways:")
    print("1. ç”Ÿäº§ç¯å¢ƒéœ€è¦å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
    print("2. ç¼“å­˜å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½")
    print("3. ç›‘æ§å’Œæ—¥å¿—å¯¹äºé—®é¢˜æ’æŸ¥å¾ˆé‡è¦")
    print("4. é¢„å–å’Œå¤šçº¿ç¨‹å¯ä»¥æé«˜ååé‡")
    print("5. DALI å¯ä»¥é«˜æ•ˆå¤„ç†å¯¹è±¡å­˜å‚¨æ•°æ®")
    print("\nğŸ‰ æ­å–œï¼ä½ å·²å®Œæˆ DALI åŸºç¡€æ•™ç¨‹")
    print("ç°åœ¨ä½ å¯ä»¥æ„å»ºé«˜æ€§èƒ½çš„æ•°æ®æµæ°´çº¿ä» MinIO åŠ è½½æ•°æ®äº†ï¼")


if __name__ == "__main__":
    main()
