#!/usr/bin/env python3
"""
DALI Agent - Natural Language Data Processing Agent

This agent understands natural language requests and automatically
calls the appropriate DALI HTTP API endpoints to configure data processing.

Usage:
    python dali_agent.py

Then interact in natural language:
    > æˆ‘éœ€è¦å‡†å¤‡ä¸€ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œæ•°æ®åœ¨ /data/imagenet è·¯å¾„ï¼Œæ‰¹æ¬¡å¤§å°32ï¼Œå›¾åƒå°ºå¯¸224x224ï¼Œéœ€è¦éšæœºè£å‰ªå’Œæ°´å¹³ç¿»è½¬
"""

import json
import re
import os
import sys
from typing import Dict, Any, List, Optional, Tuple
import requests


# ============================================================
# Configuration
# ============================================================

DALI_API_BASE = os.environ.get("DALI_API_BASE", "http://localhost:8000")
DEFAULT_BATCH_SIZE = 32
DEFAULT_IMAGE_SIZE = 224
DEFAULT_SUPPORTED_FORMATS = ["jpg", "jpeg", "png"]


# ============================================================
# DALI API Client
# ============================================================

class DALIClient:
    """Client for DALI HTTP API"""

    def __init__(self, base_url: str = DALI_API_BASE):
        self.base_url = base_url.rstrip("/")

    def health_check(self) -> Dict[str, Any]:
        """Check if API is available"""
        response = requests.get(f"{self.base_url}/health", timeout=5)
        response.raise_for_status()
        return response.json()

    def create_dataset(self, name: str, num_images: int, image_size: int) -> Dict[str, Any]:
        """Create test dataset"""
        response = requests.post(
            f"{self.base_url}/api/dataset/create",
            json={
                "name": name,
                "num_images": num_images,
                "image_size": image_size
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def import_local_dataset(
        self,
        dataset_name: str,
        local_path: str,
        supported_formats: List[str] = None
    ) -> Dict[str, Any]:
        """Import local dataset"""
        if supported_formats is None:
            supported_formats = DEFAULT_SUPPORTED_FORMATS

        response = requests.post(
            f"{self.base_url}/api/dataset/import/local",
            json={
                "dataset_name": dataset_name,
                "local_path": local_path,
                "supported_formats": supported_formats
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def import_s3_dataset(
        self,
        dataset_name: str,
        s3_uri: str,
        endpoint_url: Optional[str] = None,
        access_key: Optional[str] = None,
        secret_key: Optional[str] = None,
        download: bool = True,
        supported_formats: List[str] = None
    ) -> Dict[str, Any]:
        """Import S3 dataset"""
        if supported_formats is None:
            supported_formats = DEFAULT_SUPPORTED_FORMATS

        payload = {
            "dataset_name": dataset_name,
            "s3_uri": s3_uri,
            "download": download,
            "supported_formats": supported_formats
        }

        if endpoint_url:
            payload["endpoint_url"] = endpoint_url
        if access_key:
            payload["access_key"] = access_key
        if secret_key:
            payload["secret_key"] = secret_key

        response = requests.post(
            f"{self.base_url}/api/dataset/import/s3",
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        return response.json()

    def create_pipeline(
        self,
        name: str,
        dataset_name: str,
        pipeline_type: str,
        batch_size: int,
        target_size: int
    ) -> Dict[str, Any]:
        """Create pipeline"""
        response = requests.post(
            f"{self.base_url}/api/pipeline/create",
            json={
                "name": name,
                "dataset_name": dataset_name,
                "pipeline_type": pipeline_type,
                "batch_size": batch_size,
                "target_size": target_size
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def run_pipeline(
        self,
        pipeline_name: str,
        num_iterations: int = 1
    ) -> Dict[str, Any]:
        """Run pipeline"""
        response = requests.post(
            f"{self.base_url}/api/pipeline/run",
            json={
                "pipeline_name": pipeline_name,
                "num_iterations": num_iterations
            },
            timeout=60
        )
        response.raise_for_status()
        return response.json()

    def list_datasets(self) -> Dict[str, Any]:
        """List all datasets"""
        response = requests.get(f"{self.base_url}/api/dataset/list", timeout=5)
        response.raise_for_status()
        return response.json()

    def list_pipelines(self) -> Dict[str, Any]:
        """List all pipelines"""
        response = requests.get(f"{self.base_url}/api/pipeline/list", timeout=5)
        response.raise_for_status()
        return response.json()


# ============================================================
# Natural Language Parser
# ============================================================

class NLParser:
    """Parse natural language requests into structured parameters"""

    # Keywords for augmentation detection
    AUGMENTATION_KEYWORDS = [
        # Chinese
        "å¢å¼º", "è£å‰ª", "ç¿»è½¬", "æ—‹è½¬", "äº®åº¦", "å¯¹æ¯”åº¦",
        "éšæœº", "æ•°æ®å¢å¼º",
        # English
        "augment", "augmentation", "crop", "flip", "rotate",
        "brightness", "contrast", "random"
    ]

    # Keywords for basic processing
    BASIC_KEYWORDS = [
        # Chinese
        "åŸºç¡€", "ç®€å•", "ä»…", "åª", "ä¸éœ€è¦å¢å¼º",
        # English
        "basic", "simple", "only", "just", "no augment"
    ]

    @staticmethod
    def extract_path(text: str) -> Optional[str]:
        """Extract file path from text"""
        # Match paths like /data/imagenet, /path/to/data, etc.
        patterns = [
            r'(?:æ•°æ®åœ¨|data at|path|from)\s+([/\w\-_.]+)',
            r'([/]\w+[/\w\-_.]*)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                path = match.group(1)
                # Clean up path
                path = path.rstrip('ï¼Œã€‚,.')
                return path

        return None

    @staticmethod
    def extract_s3_uri(text: str) -> Optional[str]:
        """Extract S3 URI from text"""
        match = re.search(r's3://[\w\-./]+', text, re.IGNORECASE)
        if match:
            return match.group(0)
        return None

    @staticmethod
    def extract_batch_size(text: str) -> int:
        """Extract batch size from text"""
        patterns = [
            r'æ‰¹æ¬¡[å¤§å°]*[ï¼š:]*\s*(\d+)',
            r'batch\s*(?:size)?[ï¼š:]*\s*(\d+)',
            r'æ‰¹[ï¼š:]*\s*(\d+)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))

        return DEFAULT_BATCH_SIZE

    @staticmethod
    def extract_image_size(text: str) -> int:
        """Extract image size from text"""
        patterns = [
            r'(?:å›¾åƒ)?å°ºå¯¸[ï¼š:]*\s*(\d+)',
            r'(?:image)?\s*size[ï¼š:]*\s*(\d+)',
            r'(\d+)x\1',  # 224x224
            r'(\d+)\s*[Ã—x]\s*\1',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))

        return DEFAULT_IMAGE_SIZE

    @staticmethod
    def extract_num_images(text: str) -> Optional[int]:
        """Extract number of test images to create"""
        patterns = [
            r'(\d+)\s*å¼ ',
            r'(\d+)\s*(?:images?|pics?)',
        ]

        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))

        return None

    @staticmethod
    def detect_pipeline_type(text: str) -> str:
        """Detect if user wants basic or augmentation pipeline"""
        text_lower = text.lower()

        # Check for explicit basic keywords first
        if any(keyword in text_lower for keyword in NLParser.BASIC_KEYWORDS):
            return "basic"

        # Check for augmentation keywords
        if any(keyword in text_lower for keyword in NLParser.AUGMENTATION_KEYWORDS):
            return "augmentation"

        # Default to basic
        return "basic"

    @staticmethod
    def detect_data_source(text: str) -> Tuple[str, Optional[str]]:
        """
        Detect data source type and location

        Returns:
            (source_type, location)
            source_type: "local", "s3", "test"
            location: path, s3_uri, or None for test
        """
        # Check for S3
        s3_uri = NLParser.extract_s3_uri(text)
        if s3_uri:
            return ("s3", s3_uri)

        # Check for local path
        local_path = NLParser.extract_path(text)
        if local_path:
            return ("local", local_path)

        # Check for test data keywords
        test_keywords = ["æµ‹è¯•", "test", "synthetic", "ç”Ÿæˆ"]
        if any(keyword in text.lower() for keyword in test_keywords):
            return ("test", None)

        # Default to local with no path specified
        return ("local", None)

    @staticmethod
    def parse_request(text: str) -> Dict[str, Any]:
        """Parse complete request into parameters"""
        source_type, location = NLParser.detect_data_source(text)

        params = {
            "source_type": source_type,
            "location": location,
            "batch_size": NLParser.extract_batch_size(text),
            "image_size": NLParser.extract_image_size(text),
            "pipeline_type": NLParser.detect_pipeline_type(text),
            "num_images": NLParser.extract_num_images(text),
        }

        return params


# ============================================================
# DALI Agent
# ============================================================

class DALIAgent:
    """Main agent that orchestrates API calls based on natural language"""

    def __init__(self, api_base: str = DALI_API_BASE):
        self.client = DALIClient(api_base)
        self.parser = NLParser()

    def check_api_availability(self) -> bool:
        """Check if DALI API is available"""
        try:
            self.client.health_check()
            return True
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ° DALI API æœåŠ¡å™¨: {e}")
            print(f"   è¯·ç¡®ä¿æœåŠ¡å™¨è¿è¡Œåœ¨: {self.client.base_url}")
            print(f"   å¯åŠ¨å‘½ä»¤: python dali_http_server.py")
            return False

    def generate_dataset_name(self, location: Optional[str]) -> str:
        """Generate meaningful dataset name"""
        if location:
            if location.startswith("s3://"):
                # Extract bucket and prefix
                parts = location.replace("s3://", "").split("/")
                return f"s3_{parts[0]}"
            else:
                # Extract directory name
                name = os.path.basename(location.rstrip("/"))
                return f"{name}_dataset" if name else "local_dataset"
        else:
            return "test_dataset"

    def generate_pipeline_name(
        self,
        dataset_name: str,
        pipeline_type: str,
        batch_size: int
    ) -> str:
        """Generate meaningful pipeline name"""
        return f"{dataset_name}_{pipeline_type}_{batch_size}"

    def process_request(self, user_input: str) -> None:
        """Process user request and execute workflow"""

        print("\n" + "="*70)
        print("  æ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")
        print("="*70 + "\n")

        # Parse request
        params = self.parser.parse_request(user_input)

        print(f"ğŸ“‹ æ£€æµ‹åˆ°çš„å‚æ•°:")
        print(f"   - æ•°æ®æº: {params['source_type']}")
        if params['location']:
            print(f"   - ä½ç½®: {params['location']}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {params['batch_size']}")
        print(f"   - å›¾åƒå°ºå¯¸: {params['image_size']}x{params['image_size']}")
        print(f"   - Pipelineç±»å‹: {params['pipeline_type']}")
        if params['num_images']:
            print(f"   - å›¾åƒæ•°é‡: {params['num_images']}")

        # Step 1: Import or create dataset
        print(f"\n{'='*70}")
        print("  æ­¥éª¤ 1: å‡†å¤‡æ•°æ®é›†")
        print("="*70)

        dataset_name = self.generate_dataset_name(params['location'])

        try:
            if params['source_type'] == "test":
                # Create test dataset
                num_images = params['num_images'] or 50
                print(f"æ­£åœ¨åˆ›å»º {num_images} å¼ æµ‹è¯•å›¾åƒ...")
                result = self.client.create_dataset(
                    name=dataset_name,
                    num_images=num_images,
                    image_size=params['image_size']
                )
                print(f"âœ… æµ‹è¯•æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
                print(f"   - æ•°æ®é›†åç§°: {result['dataset_name']}")
                print(f"   - å›¾åƒæ•°é‡: {result['num_files']}")
                print(f"   - å­˜å‚¨è·¯å¾„: {result['dataset_path']}")

            elif params['source_type'] == "local":
                # Import local dataset
                if not params['location']:
                    print("âŒ é”™è¯¯: æœªæŒ‡å®šæ•°æ®è·¯å¾„")
                    print("   ç¤ºä¾‹: æ•°æ®åœ¨ /data/imagenet")
                    return

                print(f"æ­£åœ¨å¯¼å…¥æœ¬åœ°æ•°æ®é›†: {params['location']}...")
                result = self.client.import_local_dataset(
                    dataset_name=dataset_name,
                    local_path=params['location']
                )
                print(f"âœ… æœ¬åœ°æ•°æ®é›†å¯¼å…¥æˆåŠŸ")
                print(f"   - æ•°æ®é›†åç§°: {result['dataset_name']}")
                print(f"   - å›¾åƒæ•°é‡: {result['num_files']:,}")
                print(f"   - æ•°æ®è·¯å¾„: {result['dataset_path']}")

            elif params['source_type'] == "s3":
                # Import S3 dataset
                print(f"æ­£åœ¨ä» S3 å¯¼å…¥æ•°æ®é›†: {params['location']}...")
                result = self.client.import_s3_dataset(
                    dataset_name=dataset_name,
                    s3_uri=params['location'],
                    download=True
                )
                print(f"âœ… S3 æ•°æ®é›†å¯¼å…¥æˆåŠŸ")
                print(f"   - æ•°æ®é›†åç§°: {result['dataset_name']}")
                print(f"   - å›¾åƒæ•°é‡: {result['num_files']:,}")
                print(f"   - S3 URI: {result['s3_uri']}")
                if result.get('downloaded'):
                    print(f"   - æœ¬åœ°è·¯å¾„: {result['dataset_path']}")

        except requests.exceptions.HTTPError as e:
            print(f"âŒ æ•°æ®é›†å¯¼å…¥å¤±è´¥: {e.response.json().get('detail', str(e))}")
            return
        except Exception as e:
            print(f"âŒ æ•°æ®é›†å¯¼å…¥å¤±è´¥: {e}")
            return

        # Step 2: Create pipeline
        print(f"\n{'='*70}")
        print("  æ­¥éª¤ 2: åˆ›å»º Pipeline")
        print("="*70)

        pipeline_name = self.generate_pipeline_name(
            dataset_name,
            params['pipeline_type'],
            params['batch_size']
        )

        try:
            print(f"æ­£åœ¨åˆ›å»º {params['pipeline_type']} Pipeline...")
            result = self.client.create_pipeline(
                name=pipeline_name,
                dataset_name=dataset_name,
                pipeline_type=params['pipeline_type'],
                batch_size=params['batch_size'],
                target_size=params['image_size']
            )

            print(f"âœ… Pipeline åˆ›å»ºæˆåŠŸ")
            print(f"   - Pipelineåç§°: {result['pipeline_name']}")
            print(f"   - ç±»å‹: {result['pipeline_type']}")
            print(f"   - æ‰¹æ¬¡å¤§å°: {result['batch_size']}")
            print(f"   - ç›®æ ‡å°ºå¯¸: {result['target_size']}x{result['target_size']}")

            if params['pipeline_type'] == "augmentation":
                print(f"   - å¢å¼ºæ“ä½œ: éšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬ã€æ—‹è½¬ã€äº®åº¦/å¯¹æ¯”åº¦è°ƒæ•´")

        except requests.exceptions.HTTPError as e:
            print(f"âŒ Pipeline åˆ›å»ºå¤±è´¥: {e.response.json().get('detail', str(e))}")
            return
        except Exception as e:
            print(f"âŒ Pipeline åˆ›å»ºå¤±è´¥: {e}")
            return

        # Summary
        print(f"\n{'='*70}")
        print("  âœ… é…ç½®å®Œæˆï¼")
        print("="*70)
        print(f"\n**æ•°æ®é›†:** {dataset_name}")
        print(f"**Pipeline:** {pipeline_name}")
        print(f"**çŠ¶æ€:** å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ\n")
        print(f"ğŸ’¡ æç¤º:")
        print(f"   - è¿è¡Œæµ‹è¯•: python -c 'agent.run_pipeline(\"{pipeline_name}\")'")
        print(f"   - æŸ¥çœ‹æ‰€æœ‰: agent.list_resources()")
        print(f"   - åœ¨è®­ç»ƒä»£ç ä¸­å¼•ç”¨ Pipeline: '{pipeline_name}'")
        print()

    def run_pipeline_test(self, pipeline_name: str, iterations: int = 1) -> None:
        """Run pipeline test"""
        print(f"\nè¿è¡Œ Pipeline æµ‹è¯•: {pipeline_name}...")
        try:
            result = self.client.run_pipeline(pipeline_name, iterations)
            print(f"âœ… Pipeline è¿è¡ŒæˆåŠŸ")
            print(f"   - è¿­ä»£æ¬¡æ•°: {result['iterations']}")
            print(f"   - æ‰¹æ¬¡å¤§å°: {result['batch_size']}")
            for batch in result['batches'][:3]:  # Show first 3 batches
                print(f"   - Batch {batch['iteration']}: {batch['shapes']}")
        except Exception as e:
            print(f"âŒ Pipeline è¿è¡Œå¤±è´¥: {e}")

    def list_resources(self) -> None:
        """List all datasets and pipelines"""
        print("\n" + "="*70)
        print("  èµ„æºåˆ—è¡¨")
        print("="*70)

        # List datasets
        try:
            datasets = self.client.list_datasets()
            print(f"\nğŸ“¦ æ•°æ®é›† ({datasets['count']}):")
            for ds in datasets['datasets']:
                print(f"   - {ds['name']}: {ds['path']}")
        except Exception as e:
            print(f"âŒ æ— æ³•è·å–æ•°æ®é›†åˆ—è¡¨: {e}")

        # List pipelines
        try:
            pipelines = self.client.list_pipelines()
            print(f"\nğŸ”§ Pipeline ({pipelines['count']}):")
            for pipe in pipelines['pipelines']:
                print(f"   - {pipe['name']}: {pipe['type']} (batch={pipe['batch_size']})")
        except Exception as e:
            print(f"âŒ æ— æ³•è·å– Pipeline åˆ—è¡¨: {e}")

        print()


# ============================================================
# Interactive Mode
# ============================================================

def interactive_mode():
    """Run agent in interactive mode"""
    print("="*70)
    print("  DALI Agent - è‡ªç„¶è¯­è¨€æ•°æ®å¤„ç†åŠ©æ‰‹")
    print("="*70)
    print()
    print("æˆ‘å¯ä»¥å¸®ä½ é…ç½®å›¾åƒæ•°æ®é›†çš„å¤„ç†æµç¨‹ã€‚")
    print("ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½ çš„éœ€æ±‚ï¼Œæˆ‘ä¼šè‡ªåŠ¨è°ƒç”¨ DALI APIã€‚")
    print()
    print("ç¤ºä¾‹:")
    print('  > æˆ‘éœ€è¦å‡†å¤‡ä¸€ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œæ•°æ®åœ¨ /data/imagenet è·¯å¾„ï¼Œ')
    print('    æ‰¹æ¬¡å¤§å°32ï¼Œå›¾åƒå°ºå¯¸224x224ï¼Œéœ€è¦éšæœºè£å‰ªå’Œæ°´å¹³ç¿»è½¬')
    print()
    print('  > Create a test dataset with 100 images, batch 16, size 128x128')
    print()
    print('  > ä» s3://my-bucket/images å¯¼å…¥æ•°æ®ï¼Œbatch 64ï¼Œåšæ•°æ®å¢å¼º')
    print()
    print("å‘½ä»¤:")
    print("  - list: åˆ—å‡ºæ‰€æœ‰èµ„æº")
    print("  - test <pipeline_name>: æµ‹è¯•è¿è¡Œ pipeline")
    print("  - quit: é€€å‡º")
    print("="*70)
    print()

    agent = DALIAgent()

    # Check API availability
    if not agent.check_api_availability():
        return

    print("âœ… DALI API æœåŠ¡å™¨è¿æ¥æˆåŠŸ\n")

    while True:
        try:
            user_input = input("ğŸ‘¤ > ").strip()

            if not user_input:
                continue

            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nå†è§ï¼")
                break

            if user_input.lower() == 'list':
                agent.list_resources()
                continue

            if user_input.lower().startswith('test '):
                pipeline_name = user_input[5:].strip()
                agent.run_pipeline_test(pipeline_name)
                continue

            # Process natural language request
            agent.process_request(user_input)

        except KeyboardInterrupt:
            print("\n\nå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


# ============================================================
# Main
# ============================================================

if __name__ == "__main__":
    if len(sys.argv) > 1:
        # Direct command mode
        agent = DALIAgent()
        if not agent.check_api_availability():
            sys.exit(1)
        agent.process_request(" ".join(sys.argv[1:]))
    else:
        # Interactive mode
        interactive_mode()
