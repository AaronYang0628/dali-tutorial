# DALI MCP Server - å¿«é€Ÿå¼€å§‹æŒ‡å—

## âœ… å¿«é€ŸéªŒè¯

é¦–å…ˆéªŒè¯æœåŠ¡å™¨æ˜¯å¦å¯ä»¥æ­£å¸¸å·¥ä½œï¼š

```bash
cd /workspaces/dali-tutorial/mcp
python test_server.py
```

å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼Œä½ å¯ä»¥ç»§ç»­ä½¿ç”¨æœåŠ¡å™¨ã€‚

## ğŸ“¦ å®‰è£…ä¾èµ–

å¦‚æœå°šæœªå®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

## ğŸš€ ä¸‰ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1: å‘½ä»¤è¡Œå¿«é€Ÿä½“éªŒï¼ˆæ¨èæ–°æ‰‹ï¼‰

æœ€ç®€å•çš„æ–¹å¼ï¼Œè¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼š

```bash
cd /workspaces/dali-tutorial/mcp
python example_client.py
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
============================================================
DALI MCP Server ä½¿ç”¨ç¤ºä¾‹
============================================================

ğŸ“‹ æ­¥éª¤ 1: åˆ—å‡ºå¯ç”¨å·¥å…·
------------------------------------------------------------
å¯ç”¨å·¥å…·æ•°é‡: 5
  - create_test_dataset: åˆ›å»ºæµ‹è¯•å›¾åƒæ•°æ®é›†
  - create_pipeline: åˆ›å»º DALI æ•°æ®å¤„ç† Pipeline
  - run_pipeline: è¿è¡Œ DALI Pipeline å¹¶è·å–å¤„ç†ç»“æœç»Ÿè®¡
  - list_datasets: åˆ—å‡ºæ‰€æœ‰å·²åˆ›å»ºçš„æ•°æ®é›†
  - list_pipelines: åˆ—å‡ºæ‰€æœ‰å·²åˆ›å»ºçš„ Pipeline

ğŸ“¸ æ­¥éª¤ 2: åˆ›å»ºæµ‹è¯•æ•°æ®é›†
------------------------------------------------------------
{
  "dataset_name": "my_dataset",
  "dataset_path": "/tmp/dali_dataset_my_dataset_xxx",
  "num_files": 20,
  "image_size": 256,
  "file_list": [...]
}

... [æ›´å¤šè¾“å‡º] ...
```

### æ–¹å¼ 2: Python è„šæœ¬ä½¿ç”¨ï¼ˆæ¨èå¼€å‘è€…ï¼‰

åœ¨è‡ªå·±çš„ Python è„šæœ¬ä¸­ä½¿ç”¨æœåŠ¡å™¨ï¼š

```python
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
import json

async def main():
    server_params = StdioServerParameters(
        command="python",
        args=["/workspaces/dali-tutorial/mcp/dali_mcp_server.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # åˆ›å»ºæ•°æ®é›†
            print("åˆ›å»ºæ•°æ®é›†...")
            result = await session.call_tool(
                "create_test_dataset",
                arguments={
                    "name": "my_data",
                    "num_images": 100,
                    "image_size": 256
                }
            )
            print(json.dumps(json.loads(result.content[0].text), indent=2))

            # åˆ›å»º Pipeline
            print("\nåˆ›å»º Pipeline...")
            result = await session.call_tool(
                "create_pipeline",
                arguments={
                    "name": "my_pipe",
                    "dataset_name": "my_data",
                    "pipeline_type": "basic",
                    "batch_size": 8,
                    "target_size": 224
                }
            )
            print(json.dumps(json.loads(result.content[0].text), indent=2))

            # è¿è¡Œ Pipeline
            print("\nè¿è¡Œ Pipeline...")
            result = await session.call_tool(
                "run_pipeline",
                arguments={
                    "pipeline_name": "my_pipe",
                    "num_iterations": 3
                }
            )
            print(json.dumps(json.loads(result.content[0].text), indent=2))

asyncio.run(main())
```

ä¿å­˜ä¸º `my_script.py`ï¼Œè¿è¡Œï¼š
```bash
python my_script.py
```

### æ–¹å¼ 3: Claude Desktop é›†æˆï¼ˆæ¨è AI äº¤äº’ï¼‰

å°† MCP æœåŠ¡å™¨é›†æˆåˆ° Claude Desktopï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€ä¸ DALI äº¤äº’ã€‚

#### å®‰è£…æ­¥éª¤

**macOS**ï¼š
```bash
# 1. æ‰“å¼€ Claude Desktop é…ç½®æ–‡ä»¶
open ~/Library/Application\ Support/Claude/claude_desktop_config.json

# 2. è¿½åŠ ä»¥ä¸‹å†…å®¹ï¼ˆä¿ç•™ JSON æ ¼å¼ï¼‰
{
  "mcpServers": {
    "dali-server": {
      "command": "python",
      "args": [
        "/workspaces/dali-tutorial/mcp/dali_mcp_server.py"
      ]
    }
  }
}

# 3. é‡å¯ Claude Desktop
```

**Windows**ï¼š
```bash
# 1. æ‰“å¼€é…ç½®æ–‡ä»¶
notepad %APPDATA%\Claude\claude_desktop_config.json

# 2. è¿½åŠ  dali-server é…ç½®
# 3. é‡å¯ Claude Desktop
```

**Linux**ï¼š
```bash
# 1. æ‰“å¼€é…ç½®æ–‡ä»¶
nano ~/.config/Claude/claude_desktop_config.json

# 2. è¿½åŠ  dali-server é…ç½®
# 3. é‡å¯ Claude Desktop
```

#### åœ¨ Claude Desktop ä¸­ä½¿ç”¨

é‡å¯åï¼Œåœ¨ Claude çš„å¯¹è¯æ¡†ä¸­å°±å¯ä»¥ä½¿ç”¨ DALI å·¥å…·äº†ã€‚ä¾‹å¦‚ï¼š

**ä¾‹å­ 1: åŸºç¡€ä½¿ç”¨**
```
User: å¸®æˆ‘åˆ›å»ºä¸€ä¸ªåŒ…å« 50 å¼ å›¾ç‰‡çš„æ•°æ®é›†

Claude: æˆ‘æ¥å¸®ä½ åˆ›å»ºè¿™ä¸ªæ•°æ®é›†ã€‚
[è°ƒç”¨ create_test_dataset å·¥å…·]
{
  "dataset_name": "dataset",
  "num_files": 50,
  "image_size": 256,
  ...
}
```

**ä¾‹å­ 2: å®Œæ•´å·¥ä½œæµ**
```
User: æˆ‘éœ€è¦ï¼š
1. åˆ›å»º 1000 å¼ å›¾åƒçš„è®­ç»ƒæ•°æ®é›†
2. åˆ›å»ºä¸€ä¸ªæ•°æ®å¢å¼º pipelineï¼Œbatch size 32
3. è¿è¡Œ pipeline 5 æ¬¡çœ‹çœ‹æ•ˆæœ

Claude: æˆ‘æ¥å¸®ä½ å®Œæˆè¿™ä¸ªå·¥ä½œæµ...
[ä¾æ¬¡è°ƒç”¨å¯¹åº”å·¥å…·]
```

**ä¾‹å­ 3: æ•°æ®åˆ†æ**
```
User: åˆ›å»º 500 å¼ å›¾åƒçš„æ•°æ®é›†ï¼Œç„¶åè¿è¡Œä¸€ä¸ª augmentation pipelineï¼Œ
      å‘Šè¯‰æˆ‘å¤„ç†åå›¾åƒçš„ç»Ÿè®¡ç‰¹æ€§

Claude: å¥½çš„ï¼Œè®©æˆ‘åˆ›å»ºæ•°æ®é›†å’Œ pipelineï¼Œç„¶åè¿è¡Œå®ƒ...
[è°ƒç”¨å·¥å…·å¹¶åˆ†æç»“æœ]
```

## ğŸ“ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æµ‹è¯• DALI é…ç½®

```python
async def test_dali_setup():
    # åˆ›å»ºå°å‹æ•°æ®é›†
    dataset = await session.call_tool(
        "create_test_dataset",
        arguments={"name": "test", "num_images": 10}
    )

    # åˆ›å»º pipeline
    pipe = await session.call_tool(
        "create_pipeline",
        arguments={
            "name": "test_pipe",
            "dataset_name": "test",
            "pipeline_type": "basic"
        }
    )

    # å¿«é€Ÿè¿è¡Œ
    results = await session.call_tool(
        "run_pipeline",
        arguments={"pipeline_name": "test_pipe"}
    )
```

### åœºæ™¯ 2: æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
async def benchmark():
    # åˆ›å»ºå¤§æ•°æ®é›†
    await session.call_tool(
        "create_test_dataset",
        arguments={"name": "bench", "num_images": 10000}
    )

    # åˆ›å»ºä¸åŒ batch size çš„ pipeline
    for bs in [8, 16, 32, 64]:
        await session.call_tool(
            "create_pipeline",
            arguments={
                "name": f"pipe_bs{bs}",
                "dataset_name": "bench",
                "batch_size": bs
            }
        )

        # è¿è¡Œå¹¶æ¯”è¾ƒæ€§èƒ½
        results = await session.call_tool(
            "run_pipeline",
            arguments={
                "pipeline_name": f"pipe_bs{bs}",
                "num_iterations": 100
            }
        )
```

### åœºæ™¯ 3: å¯¼å…¥æœ¬åœ°æ•°æ®é›†

```python
async def import_local_data():
    # 1. ä»æœ¬åœ°ç›®å½•å¯¼å…¥çœŸå®æ•°æ®
    dataset = await session.call_tool(
        "import_local_dataset",
        arguments={
            "dataset_name": "my_photos",
            "local_path": "/home/user/dataset/photos",
            "supported_formats": ["jpg", "png"]
        }
    )
    print(f"å¯¼å…¥ {dataset['num_files']} å¼ å›¾åƒ")

    # 2. åˆ›å»ºå¤„ç† pipeline
    pipe = await session.call_tool(
        "create_pipeline",
        arguments={
            "name": "photo_pipe",
            "dataset_name": "my_photos",
            "pipeline_type": "augmentation",
            "batch_size": 16
        }
    )

    # 3. è¿è¡Œå¹¶è·å–ç»“æœ
    results = await session.call_tool(
        "run_pipeline",
        arguments={"pipeline_name": "photo_pipe", "num_iterations": 5}
    )
```

### åœºæ™¯ 4: å¯¼å…¥ S3 æ•°æ®é›†

```python
async def import_s3_data():
    # 1. ä» AWS S3 å¯¼å…¥æ•°æ®ï¼ˆä¸‹è½½åˆ°æœ¬åœ°ï¼‰
    import os

    # è®¾ç½®å‡­è¯ï¼ˆæˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
    os.environ["AWS_ACCESS_KEY_ID"] = "your_key"
    os.environ["AWS_SECRET_ACCESS_KEY"] = "your_secret"

    dataset = await session.call_tool(
        "import_s3_dataset",
        arguments={
            "dataset_name": "s3_training",
            "s3_uri": "s3://my-bucket/training-data",
            "download": True,  # ä¸‹è½½åˆ°æœ¬åœ°
            "supported_formats": ["jpg", "png"]
        }
    )
    print(f"ä¸‹è½½äº† {dataset['num_files']} å¼ å›¾åƒåˆ° {dataset['local_path']}")

    # 2. åˆ›å»ºå¤„ç† pipeline
    pipe = await session.call_tool(
        "create_pipeline",
        arguments={
            "name": "s3_pipe",
            "dataset_name": "s3_training",
            "pipeline_type": "augmentation",
            "batch_size": 32
        }
    )

    # 3. è¿è¡Œ pipeline
    results = await session.call_tool(
        "run_pipeline",
        arguments={"pipeline_name": "s3_pipe", "num_iterations": 10}
    )
```

### åœºæ™¯ 5: MinIO ç§æœ‰å­˜å‚¨é›†æˆ

```python
async def import_minio_data():
    # ä» MinIO å¯¼å…¥æ•°æ®
    dataset = await session.call_tool(
        "import_s3_dataset",
        arguments={
            "dataset_name": "minio_data",
            "s3_uri": "s3://private-bucket/images",
            "endpoint_url": "http://minio-server:9000",
            "access_key": "minioadmin",
            "secret_key": "minioadmin",
            "download": True
        }
    )
    print(f"ä» MinIO å¯¼å…¥ {dataset['num_files']} å¼ å›¾åƒ")
```

### åœºæ™¯ 6: æ•°æ®é¢„å¤„ç†å·¥ä½œæµ

```python
async def prepare_data():
    # 1. ä»æœ¬åœ°å¯¼å…¥è®­ç»ƒé›†
    train = await session.call_tool(
        "import_local_dataset",
        arguments={
            "dataset_name": "train",
            "local_path": "/data/train"
        }
    )

    # 2. ä» S3 å¯¼å…¥éªŒè¯é›†
    val = await session.call_tool(
        "import_s3_dataset",
        arguments={
            "dataset_name": "val",
            "s3_uri": "s3://my-bucket/validation",
            "download": True
        }
    )

    # 3. åˆ›å»ºè®­ç»ƒ pipelineï¼ˆæ•°æ®å¢å¼ºï¼‰
    train_pipe = await session.call_tool(
        "create_pipeline",
        arguments={
            "name": "train_pipe",
            "dataset_name": "train",
            "pipeline_type": "augmentation",
            "batch_size": 32
        }
    )

    # 4. åˆ›å»ºéªŒè¯ pipelineï¼ˆåŸºç¡€å¤„ç†ï¼‰
    val_pipe = await session.call_tool(
        "create_pipeline",
        arguments={
            "name": "val_pipe",
            "dataset_name": "val",
            "pipeline_type": "basic",
            "batch_size": 64
        }
    )

    # 5. è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = await session.call_tool(
        "run_pipeline",
        arguments={"pipeline_name": "train_pipe"}
    )
```

## ğŸ”§ å·¥å…·å‚è€ƒ

### 7 ä¸ªæ ¸å¿ƒå·¥å…·

| å·¥å…·å | åŠŸèƒ½ | ç”¨é€” |
|-------|------|------|
| `create_test_dataset` | ç”Ÿæˆæµ‹è¯•å›¾åƒ | å¿«é€Ÿåˆ›å»ºæ•°æ®é›† |
| `import_local_dataset` | å¯¼å…¥æœ¬åœ°æ•°æ® | å¯¼å…¥è‡ªå·±çš„å›¾åƒæ•°æ® |
| `import_s3_dataset` | å¯¼å…¥ S3 æ•°æ® | ä» AWS S3 æˆ– MinIO å¯¼å…¥æ•°æ® |
| `create_pipeline` | åˆ›å»º DALI Pipeline | é…ç½®æ•°æ®å¤„ç†æµç¨‹ |
| `run_pipeline` | è¿è¡Œ Pipeline | æ‰§è¡Œæ•°æ®å¤„ç†å’Œè·å–ç»Ÿè®¡ |
| `list_datasets` | åˆ—å‡ºæ•°æ®é›† | æŸ¥çœ‹å·²åˆ›å»ºçš„æ•°æ®é›† |
| `list_pipelines` | åˆ—å‡º Pipeline | æŸ¥çœ‹å·²åˆ›å»ºçš„ Pipeline |

## ğŸ“Š ç†è§£è¿”å›æ•°æ®

### create_test_dataset è¿”å›å€¼

```json
{
  "dataset_name": "my_dataset",      // æ•°æ®é›†åç§°
  "dataset_path": "/tmp/...",        // ç‰©ç†è·¯å¾„
  "num_files": 20,                   // æ–‡ä»¶æ•°é‡
  "image_size": 256,                 // å›¾åƒå°ºå¯¸
  "file_list": ["img1.jpg", ...]     // æ–‡ä»¶åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼‰
}
```

### run_pipeline è¿”å›å€¼ï¼ˆå…³é”®éƒ¨åˆ†ï¼‰

```json
{
  "pipeline_name": "my_pipe",
  "statistics": {
    "batches": [
      {
        "iteration": 1,
        "batch_size": 32,
        "sample_stats": {
          "min": 0.0,                // æœ€å°åƒç´ å€¼
          "max": 1.0,                // æœ€å¤§åƒç´ å€¼
          "mean": 0.48,              // å¹³å‡åƒç´ å€¼
          "std": 0.25                // æ ‡å‡†å·®
        }
      }
    ]
  }
}
```

## ğŸ¯ ä¸‹ä¸€æ­¥

### äº†è§£æ›´å¤š

- ğŸ“– æŸ¥çœ‹å®Œæ•´ README: `README.md`
- ğŸ”§ å­¦ä¹ å¦‚ä½•æ‰©å±•: README.md ä¸­çš„ "æ‰©å±•å¼€å‘" ç« èŠ‚
- ğŸ“ æŸ¥çœ‹æºä»£ç : `dali_mcp_server.py`

### å°è¯•æ›´å¤šåŠŸèƒ½

1. ä¿®æ”¹ `example_client.py` è¿›è¡Œå®éªŒ
2. åˆ›å»ºè‡ªå·±çš„ pipeline
3. é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­

### è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. è¿è¡Œ `test_server.py` æ£€æŸ¥ç¯å¢ƒ
2. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯çš„è¯¦ç»†æè¿°
3. æ£€æŸ¥ README.md ä¸­çš„ "æ•…éšœæ’æŸ¥" éƒ¨åˆ†

## ğŸš¦ çŠ¶æ€æŒ‡ç¤º

| ç¬¦å· | å«ä¹‰ |
|------|------|
| âœ… | æˆåŠŸ |
| âŒ | å¤±è´¥ |
| âš ï¸ | è­¦å‘Š |
| ğŸ“¸ | æ•°æ®ç›¸å…³ |
| ğŸ”§ | Pipeline ç›¸å…³ |
| â–¶ï¸ | æ‰§è¡Œ |
| ğŸ“Š | ç»Ÿè®¡/åˆ—è¡¨ |

## ğŸ’¡ Tips

1. **ä¿å­˜æ•°æ®é›†åç§°**ï¼šåˆ›å»ºæ•°æ®é›†åè®°ä½åç§°ï¼Œåç»­éœ€è¦ç”¨åˆ°
2. **Pipeline å¤ç”¨**ï¼šåŒä¸€ä¸ª pipeline å¯ä»¥å¤šæ¬¡è¿è¡Œï¼Œæ— éœ€é‡å¤åˆ›å»º
3. **æ‰¹æ¬¡å¤§å°é€‰æ‹©**ï¼š
   - å°æ•°æ®é›†ï¼ˆ<100ï¼‰ï¼šbatch_size = 4-8
   - ä¸­æ•°æ®é›†ï¼ˆ100-10Kï¼‰ï¼šbatch_size = 16-32
   - å¤§æ•°æ®é›†ï¼ˆ>10Kï¼‰ï¼šbatch_size = 64-128
4. **å†…å­˜ç®¡ç†**ï¼šæ•°æ®é›†å’Œ pipeline ä¿¡æ¯å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œç¨‹åºé€€å‡ºåè‡ªåŠ¨æ¸…ç†

---

ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸ‰
