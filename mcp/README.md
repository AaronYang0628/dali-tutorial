# DALI MCP Server

ä¸€ä¸ªåŸºäº Model Context Protocol (MCP) çš„ NVIDIA DALI æœåŠ¡å™¨ï¼Œå…è®¸ AI Agent é€šè¿‡æ ‡å‡†åè®®è°ƒç”¨ DALI è¿›è¡Œæ•°æ®ç”Ÿæˆå’Œå¤„ç†ã€‚

## åŠŸèƒ½ç‰¹æ€§

### å½“å‰åŠŸèƒ½ï¼ˆv0.2ï¼‰

- âœ… **æ•°æ®é›†åˆ›å»º**ï¼šç”Ÿæˆæµ‹è¯•å›¾åƒæ•°æ®é›†
- âœ… **æœ¬åœ°æ•°æ®å¯¼å…¥**ï¼šä»æœ¬åœ°ç›®å½•å¯¼å…¥çœŸå®å›¾åƒæ•°æ®
- âœ… **S3 æ•°æ®å¯¼å…¥**ï¼šä» AWS S3 æˆ– MinIO ç­‰å…¼å®¹å­˜å‚¨å¯¼å…¥æ•°æ®
- âœ… **Pipeline ç®¡ç†**ï¼šåˆ›å»ºå’Œç®¡ç†å¤šä¸ª DALI Pipeline
- âœ… **åŸºç¡€å¤„ç†**ï¼šå›¾åƒè§£ç ã€ç¼©æ”¾ã€è£å‰ª
- âœ… **æ•°æ®å¢å¼º**ï¼šéšæœºè£å‰ªã€ç¿»è½¬ã€é¢œè‰²è°ƒæ•´
- âœ… **ç»Ÿè®¡åˆ†æ**ï¼šè·å–å¤„ç†ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯

### è§„åˆ’åŠŸèƒ½ï¼ˆæœªæ¥ç‰ˆæœ¬ï¼‰

- ğŸ”² è‡ªå®šä¹‰ Pipeline é…ç½®
- ğŸ”² æ”¯æŒæ›´å¤šæ•°æ®æ ¼å¼ï¼ˆè§†é¢‘ã€éŸ³é¢‘ï¼‰
- ğŸ”² æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
- ğŸ”² ä¸ PyTorch/TensorFlow é›†æˆ
- ğŸ”² åˆ†å¸ƒå¼å¤„ç†æ”¯æŒ

## å®‰è£…

### å‰ç½®è¦æ±‚

```bash
# 1. Python 3.8+
python --version

# 2. NVIDIA DALI
pip install nvidia-dali-cuda120

# 3. MCP SDK
pip install mcp

# 4. å…¶ä»–ä¾èµ–
pip install numpy pillow
```

### å¿«é€Ÿå®‰è£…

```bash
cd /workspaces/dali-tutorial/mcp
pip install -r requirements.txt
```

## ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1: å‘½ä»¤è¡Œå®¢æˆ·ç«¯

è¿è¡Œç¤ºä¾‹å®¢æˆ·ç«¯ï¼š

```bash
python example_client.py
```

### æ–¹å¼ 2: Claude Desktop é›†æˆ

1. **å¤åˆ¶é…ç½®åˆ° Claude Desktop**

   ```bash
   # macOS
   cat claude_desktop_config.json >> ~/Library/Application\ Support/Claude/claude_desktop_config.json

   # Windows
   type claude_desktop_config.json >> %APPDATA%\Claude\claude_desktop_config.json

   # Linux
   cat claude_desktop_config.json >> ~/.config/Claude/claude_desktop_config.json
   ```

2. **é‡å¯ Claude Desktop**

3. **åœ¨å¯¹è¯ä¸­ä½¿ç”¨å·¥å…·**

   ç°åœ¨å¯ä»¥ç›´æ¥åœ¨ Claude Desktop ä¸­ä½¿ç”¨ DALI å·¥å…·ï¼š

   ```
   User: å¸®æˆ‘åˆ›å»ºä¸€ä¸ªåŒ…å« 50 å¼ å›¾ç‰‡çš„æ•°æ®é›†ï¼Œç„¶åç”¨æ•°æ®å¢å¼º pipeline å¤„ç†å®ƒä»¬

   Claude: æˆ‘æ¥å¸®ä½ æ“ä½œï¼š
   1. é¦–å…ˆåˆ›å»ºæ•°æ®é›†...
   [è°ƒç”¨ create_test_dataset]
   2. åˆ›å»ºæ•°æ®å¢å¼º pipeline...
   [è°ƒç”¨ create_pipeline]
   3. è¿è¡Œ pipeline...
   [è°ƒç”¨ run_pipeline]
   ```

### æ–¹å¼ 3: Python API

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def use_dali_server():
    server_params = StdioServerParameters(
        command="python",
        args=["dali_mcp_server.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # åˆ›å»ºæ•°æ®é›†
            result = await session.call_tool(
                "create_test_dataset",
                arguments={"name": "my_data", "num_images": 100}
            )

            # ... æ›´å¤šæ“ä½œ
```

## å¯ç”¨å·¥å…·

### 1. create_test_dataset

åˆ›å»ºæµ‹è¯•å›¾åƒæ•°æ®é›†ã€‚

**å‚æ•°**ï¼š
- `name` (string, required): æ•°æ®é›†åç§°
- `num_images` (integer, optional): å›¾åƒæ•°é‡ï¼Œé»˜è®¤ 10
- `image_size` (integer, optional): å›¾åƒå°ºå¯¸ï¼Œé»˜è®¤ 256

**ç¤ºä¾‹**ï¼š
```json
{
  "name": "training_data",
  "num_images": 1000,
  "image_size": 512
}
```

**è¿”å›**ï¼š
```json
{
  "dataset_name": "training_data",
  "dataset_path": "/tmp/dali_dataset_training_data_xxx",
  "num_files": 1000,
  "image_size": 512,
  "file_list": ["...", "..."]
}
```

### 2. create_pipeline

åˆ›å»º DALI æ•°æ®å¤„ç† Pipelineã€‚

**å‚æ•°**ï¼š
- `name` (string, required): Pipeline åç§°
- `dataset_name` (string, required): æ•°æ®é›†åç§°
- `pipeline_type` (string, optional): ç±»å‹ ('basic' æˆ– 'augmentation')ï¼Œé»˜è®¤ 'basic'
- `batch_size` (integer, optional): æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ 4
- `target_size` (integer, optional): ç›®æ ‡å°ºå¯¸ï¼Œé»˜è®¤ 224

**ç¤ºä¾‹**ï¼š
```json
{
  "name": "train_pipeline",
  "dataset_name": "training_data",
  "pipeline_type": "augmentation",
  "batch_size": 32,
  "target_size": 224
}
```

**è¿”å›**ï¼š
```json
{
  "pipeline_name": "train_pipeline",
  "pipeline_type": "augmentation",
  "batch_size": 32,
  "target_size": 224,
  "dataset_name": "training_data",
  "num_files": 1000,
  "status": "created and built"
}
```

### 3. run_pipeline

è¿è¡Œ Pipeline å¹¶è·å–ç»Ÿè®¡ä¿¡æ¯ã€‚

**å‚æ•°**ï¼š
- `pipeline_name` (string, required): Pipeline åç§°
- `num_iterations` (integer, optional): è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ 1

**ç¤ºä¾‹**ï¼š
```json
{
  "pipeline_name": "train_pipeline",
  "num_iterations": 5
}
```

**è¿”å›**ï¼š
```json
{
  "pipeline_name": "train_pipeline",
  "pipeline_type": "augmentation",
  "batch_size": 32,
  "statistics": {
    "iterations": 5,
    "batches": [
      {
        "iteration": 1,
        "batch_size": 32,
        "shapes": ["(3, 224, 224)", "..."],
        "dtype": "DALIDataType.FLOAT",
        "sample_stats": {
          "min": 0.0,
          "max": 1.0,
          "mean": 0.48,
          "std": 0.25
        }
      }
    ]
  }
}
```

### 4. list_datasets

åˆ—å‡ºæ‰€æœ‰å·²åˆ›å»ºçš„æ•°æ®é›†ã€‚

**å‚æ•°**ï¼šæ— 

**è¿”å›**ï¼š
```json
{
  "count": 2,
  "datasets": [
    {
      "name": "training_data",
      "path": "/tmp/dali_dataset_training_data_xxx",
      "num_files": 1000
    },
    {
      "name": "validation_data",
      "path": "/tmp/dali_dataset_validation_data_xxx",
      "num_files": 200
    }
  ]
}
```

### 5. list_pipelines

åˆ—å‡ºæ‰€æœ‰å·²åˆ›å»ºçš„ Pipelineã€‚

**å‚æ•°**ï¼šæ— 

**è¿”å›**ï¼š
```json
{
  "count": 2,
  "pipelines": [
    {
      "name": "train_pipeline",
      "type": "augmentation",
      "batch_size": 32,
      "target_size": 224,
      "dataset_name": "training_data",
      "num_files": 1000
    },
    {
      "name": "val_pipeline",
      "type": "basic",
      "batch_size": 64,
      "target_size": 224,
      "dataset_name": "validation_data",
      "num_files": 200
    }
  ]
}
```

### 6. import_local_dataset

ä»æœ¬åœ°æ–‡ä»¶ç›®å½•å¯¼å…¥å›¾åƒæ•°æ®é›†ã€‚ç›¸æ¯” `create_test_dataset` ç”Ÿæˆéšæœºå›¾åƒï¼Œæ­¤å·¥å…·ç”¨äºå¯¼å…¥ä½ è‡ªå·±çš„çœŸå®æ•°æ®ã€‚

**å‚æ•°**ï¼š
- `dataset_name` (string, required): æ•°æ®é›†åç§°ï¼Œç”¨äºåç»­å¼•ç”¨
- `local_path` (string, required): æœ¬åœ°ç›®å½•çš„ç»å¯¹è·¯å¾„
- `supported_formats` (array, optional): æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œé»˜è®¤ `["jpg", "jpeg", "png"]`

**ç¤ºä¾‹**ï¼š
```json
{
  "dataset_name": "my_photos",
  "local_path": "/data/photos",
  "supported_formats": ["jpg", "png"]
}
```

**è¿”å›**ï¼š
```json
{
  "dataset_name": "my_photos",
  "dataset_path": "/data/photos",
  "num_files": 1250,
  "supported_formats": ["jpg", "png"],
  "file_list": [
    "/data/photos/photo_001.jpg",
    "/data/photos/photo_002.jpg",
    "..."
  ]
}
```

**å¸¸è§åœºæ™¯**ï¼š
- å¯¼å…¥è‡ªå·±çš„å›¾åƒæ•°æ®é›†ç”¨äºè®­ç»ƒ
- æ”¯æŒå¤šç§å›¾åƒæ ¼å¼è‡ªåŠ¨æ‰«æ
- ä¸ `create_pipeline` æ— ç¼é›†æˆ

**é”™è¯¯å¤„ç†**ï¼š
- è·¯å¾„ä¸å­˜åœ¨æ—¶è¿”å›é”™è¯¯
- è·¯å¾„å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„
- æ‰¾ä¸åˆ°æ”¯æŒæ ¼å¼çš„å›¾åƒæ—¶è¿”å›é”™è¯¯
- æ•°æ®é›†åç§°é‡å¤æ—¶è¿”å›é”™è¯¯

### 7. import_s3_dataset

ä» S3 å…¼å®¹å­˜å‚¨ï¼ˆAWS S3ã€MinIO ç­‰ï¼‰å¯¼å…¥å›¾åƒæ•°æ®é›†ã€‚æ”¯æŒåˆ—ä¸¾æˆ–ä¸‹è½½ä¸¤ç§æ¨¡å¼ã€‚

**å‚æ•°**ï¼š
- `dataset_name` (string, required): æ•°æ®é›†åç§°
- `s3_uri` (string, required): S3 URIï¼Œæ ¼å¼ `s3://bucket/prefix` æˆ– `s3://bucket`
- `endpoint_url` (string, optional): S3 ç«¯ç‚¹ URLï¼ˆç”¨äº MinIO ç­‰å…¼å®¹å­˜å‚¨ï¼‰
- `access_key` (string, optional): AWS access keyï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡ `AWS_ACCESS_KEY_ID` è¯»å–ï¼‰
- `secret_key` (string, optional): AWS secret keyï¼ˆä¼˜å…ˆä»ç¯å¢ƒå˜é‡ `AWS_SECRET_ACCESS_KEY` è¯»å–ï¼‰
- `download` (boolean, optional): æ˜¯å¦ä¸‹è½½åˆ°æœ¬åœ°ï¼Œé»˜è®¤ `false`
- `supported_formats` (array, optional): æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œé»˜è®¤ `["jpg", "jpeg", "png"]`

**ç¤ºä¾‹ 1ï¼šAWS S3 + ä¸‹è½½**ï¼š
```json
{
  "dataset_name": "s3_training_data",
  "s3_uri": "s3://my-bucket/datasets/training",
  "download": true,
  "supported_formats": ["jpg", "png"]
}
```

**ç¤ºä¾‹ 2ï¼šMinIO + æµå¼è¯»å–**ï¼š
```json
{
  "dataset_name": "minio_data",
  "s3_uri": "s3://data-bucket/images",
  "endpoint_url": "http://minio:9000",
  "access_key": "minioadmin",
  "secret_key": "minioadmin",
  "download": false
}
```

**è¿”å›ï¼ˆä¸‹è½½æ¨¡å¼ï¼‰**ï¼š
```json
{
  "dataset_name": "s3_training_data",
  "s3_uri": "s3://my-bucket/datasets/training",
  "num_files": 5000,
  "local_path": "/tmp/dali_s3_dataset_s3_training_data_xxx",
  "status": "downloaded",
  "file_list": ["image_001.jpg", "image_002.jpg", "..."]
}
```

**è¿”å›ï¼ˆæµå¼è¯»å–æ¨¡å¼ï¼‰**ï¼š
```json
{
  "dataset_name": "minio_data",
  "s3_uri": "s3://data-bucket/images",
  "num_files": 3000,
  "status": "listed",
  "note": "Files not downloaded. Use download=true to download to local directory before creating pipeline.",
  "file_list": ["img_001.jpg", "img_002.jpg", "..."]
}
```

**å‡­è¯ç®¡ç†**ï¼š
1. **ä¼˜å…ˆçº§é¡ºåº**ï¼š
   - ç¯å¢ƒå˜é‡ `AWS_ACCESS_KEY_ID` å’Œ `AWS_SECRET_ACCESS_KEY` ï¼ˆæ¨èï¼‰
   - å‡½æ•°å‚æ•°ä¸­çš„ `access_key` å’Œ `secret_key` ï¼ˆå¤‡é€‰ï¼‰

2. **è®¾ç½®ç¯å¢ƒå˜é‡**ï¼š
   ```bash
   export AWS_ACCESS_KEY_ID="your_access_key"
   export AWS_SECRET_ACCESS_KEY="your_secret_key"
   ```

**ä¸¤ç§ä½¿ç”¨æ¨¡å¼**ï¼š

**æ¨¡å¼ 1ï¼šä¸‹è½½ï¼ˆ`download=true`ï¼‰**
- é€‚åˆï¼šä¸­ç­‰å¤§å°çš„æ•°æ®é›†ï¼Œéœ€è¦ Pipeline å¤„ç†
- ä¼˜ç‚¹ï¼šä¸æœ¬åœ°æ•°æ®é›†å®Œå…¨ç›¸åŒçš„ä½¿ç”¨ä½“éªŒ
- ç¼ºç‚¹ï¼šå ç”¨æœ¬åœ°ç£ç›˜ç©ºé—´
- è‡ªåŠ¨æ¸…ç†ï¼šä¸‹è½½çš„æ–‡ä»¶åœ¨æœåŠ¡å™¨å…³é—­æ—¶è‡ªåŠ¨æ¸…ç†

**æ¨¡å¼ 2ï¼šæµå¼è¯»å–ï¼ˆ`download=false`ï¼‰**
- é€‚åˆï¼šä»…æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨ï¼Œè¯„ä¼°æ•°æ®
- ä¼˜ç‚¹ï¼šèŠ‚çœç£ç›˜ç©ºé—´
- ç¼ºç‚¹ï¼šä¸èƒ½ç”¨äº Pipelineï¼ˆPipeline éœ€è¦æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼‰
- ç”¨é€”ï¼šæ•°æ®æ¢ç´¢å’Œè§„åˆ’

**å¸¸è§åœºæ™¯**ï¼š
- å¯¼å…¥ AWS S3 ä¸Šçš„å¤§è§„æ¨¡æ•°æ®é›†
- ä¸ MinIO ç§æœ‰å­˜å‚¨é›†æˆ
- æ”¯æŒå¤šä¸ªæ•°æ®æºå…±å­˜ï¼ˆæœ¬åœ°+S3ï¼‰

**é”™è¯¯å¤„ç†**ï¼š
- å‡­è¯æ— æ•ˆæ—¶è¿”å›è®¤è¯é”™è¯¯
- Bucket ä¸å­˜åœ¨æ—¶è¿”å› "NoSuchBucket" é”™è¯¯
- æƒé™ä¸è¶³æ—¶è¿”å› "AccessDenied" é”™è¯¯
- ä¸æ”¯æŒçš„æ ¼å¼çš„æ–‡ä»¶è¢«è‡ªåŠ¨è¿‡æ»¤
- boto3 æœªå®‰è£…æ—¶æç¤ºå®‰è£…

## Pipeline ç±»å‹è¯´æ˜

### basic (åŸºç¡€å¤„ç†)

æ‰§è¡Œæ“ä½œï¼š
1. å›¾åƒè§£ç ï¼ˆJPEGï¼‰
2. ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
3. ä¸­å¿ƒè£å‰ªåˆ°ç›®æ ‡å°ºå¯¸
4. è¾“å‡º HWC æ ¼å¼ï¼Œuint8 ç±»å‹

é€‚ç”¨åœºæ™¯ï¼š
- éªŒè¯é›†/æµ‹è¯•é›†å¤„ç†
- æ¨ç†é˜¶æ®µ
- ä¸éœ€è¦æ•°æ®å¢å¼ºçš„åœºæ™¯

### augmentation (æ•°æ®å¢å¼º)

æ‰§è¡Œæ“ä½œï¼š
1. å›¾åƒè§£ç ï¼ˆJPEGï¼‰
2. éšæœºç¼©æ”¾è£å‰ªï¼ˆ8%-100% é¢ç§¯ï¼‰
3. éšæœºæ°´å¹³ç¿»è½¬ï¼ˆ50% æ¦‚ç‡ï¼‰
4. äº®åº¦å’Œå¯¹æ¯”åº¦è°ƒæ•´ï¼ˆÂ±20%ï¼‰
5. å½’ä¸€åŒ–åˆ° [0, 1]
6. è½¬æ¢ä¸º CHW æ ¼å¼ï¼ˆPyTorch å…¼å®¹ï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- è®­ç»ƒé›†å¤„ç†
- éœ€è¦æ•°æ®å¢å¼ºçš„åœºæ™¯
- ä¸ PyTorch é›†æˆ

## å®Œæ•´ä½¿ç”¨æµç¨‹ç¤ºä¾‹

### åœºæ™¯ï¼šè®­ç»ƒæ•°æ®å‡†å¤‡

```python
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def prepare_training_data():
    server_params = StdioServerParameters(
        command="python",
        args=["dali_mcp_server.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # 1. åˆ›å»ºè®­ç»ƒæ•°æ®é›†
            await session.call_tool(
                "create_test_dataset",
                arguments={
                    "name": "train_set",
                    "num_images": 1000,
                    "image_size": 512
                }
            )

            # 2. åˆ›å»ºéªŒè¯æ•°æ®é›†
            await session.call_tool(
                "create_test_dataset",
                arguments={
                    "name": "val_set",
                    "num_images": 200,
                    "image_size": 512
                }
            )

            # 3. åˆ›å»ºè®­ç»ƒ Pipelineï¼ˆæ•°æ®å¢å¼ºï¼‰
            await session.call_tool(
                "create_pipeline",
                arguments={
                    "name": "train_pipe",
                    "dataset_name": "train_set",
                    "pipeline_type": "augmentation",
                    "batch_size": 32,
                    "target_size": 224
                }
            )

            # 4. åˆ›å»ºéªŒè¯ Pipelineï¼ˆåŸºç¡€å¤„ç†ï¼‰
            await session.call_tool(
                "create_pipeline",
                arguments={
                    "name": "val_pipe",
                    "dataset_name": "val_set",
                    "pipeline_type": "basic",
                    "batch_size": 64,
                    "target_size": 224
                }
            )

            # 5. æµ‹è¯•è¿è¡Œ
            train_stats = await session.call_tool(
                "run_pipeline",
                arguments={
                    "pipeline_name": "train_pipe",
                    "num_iterations": 10
                }
            )

            print("è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆï¼")
            print(train_stats.content[0].text)

asyncio.run(prepare_training_data())
```

## æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MCP Client (Agent)                    â”‚
â”‚  (Claude Desktop / Python Script / Custom Application)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ MCP Protocol (JSON-RPC)
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DALI MCP Server                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Tool Handlers                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€ create_test_dataset                         â”‚   â”‚
â”‚  â”‚  â”œâ”€ create_pipeline                             â”‚   â”‚
â”‚  â”‚  â”œâ”€ run_pipeline                                â”‚   â”‚
â”‚  â”‚  â”œâ”€ list_datasets                               â”‚   â”‚
â”‚  â”‚  â””â”€ list_pipelines                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  State Management                              â”‚     â”‚
â”‚  â”‚  â”œâ”€ datasets: Dict[name -> path]              â”‚     â”‚
â”‚  â”‚  â”œâ”€ pipelines: Dict[name -> pipeline]         â”‚     â”‚
â”‚  â”‚  â””â”€ temp_dirs: List[path]                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                        â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  DALI Pipelines                                â”‚     â”‚
â”‚  â”‚  â”œâ”€ basic_image_pipeline                      â”‚     â”‚
â”‚  â”‚  â””â”€ augmentation_pipeline                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               NVIDIA DALI Library                        â”‚
â”‚  (GPU-accelerated data loading and preprocessing)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: MCP SDK å¯¼å…¥é”™è¯¯

**é”™è¯¯ä¿¡æ¯**ï¼š
```
ModuleNotFoundError: No module named 'mcp'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
pip install mcp
```

### é—®é¢˜ 2: DALI æœªå®‰è£…æˆ–ç‰ˆæœ¬ä¸å…¼å®¹

**é”™è¯¯ä¿¡æ¯**ï¼š
```
ModuleNotFoundError: No module named 'nvidia.dali'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
pip install nvidia-dali-cuda120
# æˆ–æ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©
pip install nvidia-dali-cuda118
```

### é—®é¢˜ 3: GPU ä¸å¯ç”¨

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: CUDA not available
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
æœåŠ¡å™¨ä¼šè‡ªåŠ¨ä½¿ç”¨ CPU æ¨¡å¼ï¼Œä½†æ€§èƒ½ä¼šé™ä½ã€‚ç¡®ä¿ï¼š
1. å®‰è£…äº†æ­£ç¡®çš„ CUDA ç‰ˆæœ¬
2. GPU é©±åŠ¨æ­£å¸¸å·¥ä½œ
3. `nvidia-smi` å‘½ä»¤å¯ç”¨

### é—®é¢˜ 4: Pipeline æ„å»ºå¤±è´¥

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: Critical error when building pipeline
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åˆ›å»º
2. ç¡®è®¤æ–‡ä»¶åˆ—è¡¨ä¸ä¸ºç©º
3. æ£€æŸ¥ GPU å†…å­˜æ˜¯å¦è¶³å¤Ÿ
4. å°è¯•å‡å° batch_size

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„ Pipeline ç±»å‹

1. **å®šä¹‰ Pipeline å‡½æ•°**ï¼š

```python
@pipeline_def
def custom_pipeline(file_list, **kwargs):
    images, labels = fn.readers.file(files=file_list)
    # ä½ çš„è‡ªå®šä¹‰å¤„ç†
    images = fn.your_custom_operation(images)
    return images, labels
```

2. **åœ¨ create_pipeline ä¸­æ³¨å†Œ**ï¼š

```python
async def handle_create_pipeline(arguments: Dict[str, Any]):
    # ...
    elif pipeline_type == "custom":
        pipe = custom_pipeline(
            file_list=file_list,
            **custom_kwargs
        )
    # ...
```

3. **æ›´æ–°å·¥å…·æè¿°**ï¼š

åœ¨ `list_tools()` ä¸­æ·»åŠ æ–°çš„ pipeline_type åˆ° enumã€‚

### æ·»åŠ æ–°å·¥å…·

1. **å®šä¹‰å·¥å…·æè¿°**ï¼š

```python
@app.list_tools()
async def list_tools() -> list[Tool]:
    return [
        # ... ç°æœ‰å·¥å…·
        Tool(
            name="your_new_tool",
            description="å·¥å…·æè¿°",
            inputSchema={
                # JSON Schema
            }
        )
    ]
```

2. **å®ç°å¤„ç†å‡½æ•°**ï¼š

```python
async def handle_your_new_tool(arguments: Dict[str, Any]):
    # å®ç°é€»è¾‘
    return [TextContent(
        type="text",
        text=json.dumps(result, indent=2)
    )]
```

3. **åœ¨ call_tool ä¸­æ³¨å†Œ**ï¼š

```python
@app.call_tool()
async def call_tool(name: str, arguments: Any):
    if name == "your_new_tool":
        return await handle_your_new_tool(arguments)
    # ...
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ® GPU å†…å­˜è°ƒæ•´ batch_size
2. **çº¿ç¨‹æ•°**ï¼š`num_threads` è®¾ç½®ä¸º CPU æ ¸å¿ƒæ•°çš„ 2-4 å€
3. **é¢„å–**ï¼šDALI ä¼šè‡ªåŠ¨é¢„å–æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
4. **æ··åˆè®¾å¤‡**ï¼šä½¿ç”¨ `device="mixed"` è¿›è¡Œ GPU è§£ç 

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®æ–°åŠŸèƒ½ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## è®¸å¯è¯

MIT License

## ç›¸å…³é“¾æ¥

- [NVIDIA DALI å®˜æ–¹æ–‡æ¡£](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/)
- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)
- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)

## æ›´æ–°æ—¥å¿—

### v0.1.0 (2026-01-13)

- âœ¨ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- âœ… æ”¯æŒæ•°æ®é›†åˆ›å»º
- âœ… åŸºç¡€å’Œå¢å¼º Pipeline
- âœ… ç»Ÿè®¡ä¿¡æ¯è¾“å‡º
- âœ… Claude Desktop é›†æˆ
